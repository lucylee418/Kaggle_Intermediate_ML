{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre. Obtain a list of all of the categorical variables\n",
    "```python\n",
    "    s = (X_train.dtypes == 'object')\n",
    "    object_cols = list(s[s].index)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1. Drop Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This approach will only work well if the columns did not contain useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "    drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2. Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* assigns each unique value to a different integer.\n",
    "* In many times, validation datat contains values that don't also appear in the training data and this will cause an error.\n",
    "* There are many approaches to fixing this issue.<br>\n",
    "&nbsp; - you can write a custom ordinal encoder to deal with new categories.<br>\n",
    "&nbsp; - or, you can drop the problematic categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # Categorical columns in the training data\n",
    "    object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "    # Columns that can be safely ordinal encoded\n",
    "    good_label_cols = [col for col in object_cols if \n",
    "                    set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "            \n",
    "    # Problematic columns that will be dropped from the dataset\n",
    "    bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "\n",
    "    # Drop categorical columns that will not be encoded\n",
    "    label_X_train = X_train.drop(bad_label_cols, axis=1)\n",
    "    label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n",
    "\n",
    "    # Apply ordinal encoder \n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    label_X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])\n",
    "    label_X_valid[good_label_cols] = ordinal_encoder.transform(X_valid[good_label_cols])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* creates new columns indicating the presence (or absence) of each possible value in the original data.\n",
    "* does not assume an ordering of the categories.\n",
    "* works particularly well if there is no clear ordering in the categorical data.\n",
    "* does not perform well if the categorical variable takes on a large number of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # cf) handle_unknown='ignore' : avoid errors when the validation data contains classes that aren't represented in the training data\n",
    "    # cf) sparse=False : ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    # Apply one-hot encoder to each column with categorical data\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "    OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "    # One-hot encoding removed index; put it back\n",
    "    OH_cols_train.index = X_train.index\n",
    "    OH_cols_valid.index = X_valid.index\n",
    "\n",
    "    # Remove categorical columns (will replace with one-hot encoding)\n",
    "    num_X_train = X_train.drop(object_cols, axis=1)\n",
    "    num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "    # Add one-hot encoded columns to numerical features\n",
    "    OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "    OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check the number of unique values per column before One-Hot Encoding\n",
    "```python\n",
    "    # Get number of unique entries in each column with categorical data\n",
    "    object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "    d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "    # Print number of unique entries by column, in ascending order\n",
    "    sorted(d.items(), key=lambda x: x[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Separate low cardinality columns from high cardinality columns\n",
    "```python\n",
    "    # Columns that will be one-hot encoded\n",
    "    low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "    # Columns that will be dropped from the dataset\n",
    "    high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
