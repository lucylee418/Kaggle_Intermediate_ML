{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits\n",
    "1. Cleaner Code: Accounting for data at each step of preprocessing can get messy. With a pipeline, you won't need to manually keep track of your training and validation data at each step.\n",
    "2. Fewer Bugs: There are fewer opportunities to misapply a step or forget a preprocessing step.\n",
    "3. Easier to Productionize: It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.\n",
    "4. More Options for Model Validation: You will see an example in the next tutorial, which covers cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Define Preprocessing Steps\n",
    ": Similar to how a pipeline bundles together preprocessing and modeling steps, we use the *ColumnTransformer* class to bundle together different preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    # Preprocessing for numerical data\n",
    "    numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "    # Preprocessing for categorical data\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Create and Evaluate the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": We use the *Pipeline* class to define a pipeline that bundles the preprocessing and modeling steps. There are a few important things to notice:\n",
    "* With the pipeline, we preprocess the training data and fit the model in a single line of code. (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!)\n",
    "* With the pipeline, we supply the unprocessed features in X_valid to the predict() command, and the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    # Bundle preprocessing and modeling code in a pipeline\n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('model', model)\n",
    "                                ])\n",
    "\n",
    "    # Preprocessing of training data, fit model \n",
    "    my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Preprocessing of validation data, get predictions\n",
    "    preds = my_pipeline.predict(X_valid)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
